# Final_Airflow_project
Контейнер с инициализацией БД и ETL процессом

Сам проект содержит в себе: папку с контейнером, папку с отладочным проектом pycharm и тестовый ноутбук юпитер.
Достаточно запустить контейнер в папке airflow_DE (т.к. проект учебный все даги разовые)

## Постановка задачи:
1.Проведите подготовительную работу (один час):  
 - Прочитайте предоставленный датасет.
 - Ознакомьтесь с описаниями представленных атрибутов.
 - Оцените полноту и чистоту данных. Попытайтесь понять, что стоит за этими данными в реальном мире. Приведите данные в удобный/нормальный вид для дальнейшей работы.
   
2.Проведите разведочный анализ данных (четыре часа):  
- Проведите базовую чистку (дубликаты, пустые значения, типизация данных, ненужные атрибуты).
- Посмотрите на распределение ключевых атрибутов, их отношения.
  
3.Выполните задание согласно вашей специализации (18 часов):  
- Настройте и запустите локальную БД, подходящую для хранения и исполнения запросов к данным в предоставленном датасете.
- Создайте объекты в БД для хранения данных исходного файла.
- Обработайте и поместите в БД данные из предоставленного основного датасета.
- Настройте пайплайн сбора, обработки и записи в БД новых .json-файлов. 

Что делает контейнер:
1. Инициализирует БД Postgre, создает в ней 2 таблицы
2. Создает 2 дага airflow, первый заливает начальные данные csv в таблицы, второй обрабатывает jsonы через pickls и отправляет в БД.
3. Также для удобства добавил в контейнер adminer(http://localhost:9999) (pgsql=postgres username=airflow db=airflow password=airflow), для проверки таблиц можно выполнить запрос:
   
   ```
   select count(*) from ga_sessions
   union all
   select count(*)from ga_hits
   ```

